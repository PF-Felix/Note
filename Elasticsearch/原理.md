# 🥇基本概念

- 索引：相当于关系型数据库中表的概念
- 索引类型：ES7 移除了 type 的概念，现在直接建议写成 `_doc`，但并没有禁止使用别的类型（ES 会给一个警告）
- 文档：一个文档类似于关系型数据库中的一行数据，每个文档都有一个ID
- 字段：文档中的字段相当于关系型数据库表中的列
- 映射（mapping）：定义了文档包含哪些字段、字段类型、分析器、过滤器（字符过滤器、令牌过滤器）、分词器等

# 🥇倒排索引

上面已经说过：ES 的索引相当于关系型数据库的表

ES 对索引的每个字段都做了倒排索引

倒排索引通过分词策略，形成了词项和文章的映射关系表

倒排索引包括词项字典（Term Dictionary）和倒排列表（Posting List）
![](4427fc8387fc8ef7ae229d04ae3914ff.png)

- 词项字典：词项的集合，存储在 tim 文件中
- 词项：一段文本经过分析器分析之后得到一个个词项，每个词项指向一个倒排列表
- 倒排列表：记录词项在所有文档中出现的位置即ID、词频、偏移量；所有词项的倒排列表顺序存储在磁盘文件中

1. ES 可以像 Mysql 一样，使用 B+树建立索引字典指向倒排列表，这样就可以有 Mysql 一样的查询时间复杂度
2. 为了少读磁盘（索引存在磁盘），就必须将索引放到内存，B+树存储词项字典太大了，于是就有了 Term Index
    1. ES使用**Term Index**数据结构是 FST，是一个有向无环图，存储于 tip 文件中，需要内存加载
    2. 1、因为在内存中所以查询速度快
    3. 占用空间小，通过对单词前缀和后缀的重复利用，极大的压缩了存储空间
    4. FST 存储着词项在字典中的位置，在 FST 匹配到词项之后再去词项字典（磁盘 tim 文件）中查找词项，大大减少随机IO的次数

其他
1、倒排索引是通过 value 找 key，正向索引是通过 key 找 value
2、倒排列表如果不压缩将非常占用磁盘空间，ES 提供的压缩算法有 FOR 和 RBM

# 🥇深分页问题

其实 mysql 也存在这个问题，mysql 怎么解决呢？TODO

一般情况下分页查询使用**from+size**

```shell
#1、各个分片先查询排序前1005条数据
#2、聚合所有分片的查询结果（1005*分片数量），再次排序之后得到相应的查询结果
#如果分页太深，即 from 太大，堆内存中汇总的数据就过多，就容易出现内存溢出，这就是深分页问题
#为了防止内存溢出的问题，有一个参数max_result_window默认值是10000，来限制分页返回的最大数值
GET gpf/_search
{
  "from": 1000,
  "size": 5
}
```

解决方案看下文

**Search Scroll**

本质是第一次查询所有的数据并存为快照，之后的查询是查快照，因此是非实时的
而且一个时间点如果存在过多的 Scroll 依然会占用大量内存，因此只适用于非C端场景（C端是消费者，B端是企业）

使用方法：

```shell
GET /product/_search?scroll=1m
{
  "size": 2
}

GET /_search/scroll
{
  "scroll":"1m",
  "scroll_id":"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAADbcWZEs2eGgxaXFRWUtmSWc3Yk8xTHZTZw**"
}
```

**Search After**

原理：跟`from+size`比较类似
区别在于：每个分片查询的不是前 N 个记录，而是`search after`之后的`size`个记录，因此占用内存有限
缺点：只支持下一页查询

使用方法：

```shell
POST twitter/_search
{
    "size": 10,
    "query": {
        "match" : {
            "title" : "es"
        }
    },
    "sort": [
        {"date": "asc"},
        {"_id": "desc"}
    ]
}

GET twitter/_search
{
    "size": 10,
    "query": {
        "match" : {
            "title" : "es"
        }
    },
    "search_after": [124648691, "624812"], #上次查询最后一条记录的sort值
    "sort": [
        {"date": "asc"},
        {"_id": "desc"}
    ]
}
```

**分页功能怎么实现？**

其他考虑下来也就几个功能点：

- 自定义跳页功能：没有这个功能是可以的，并没有牺牲用户体验，因为用户关注的是搜索精准度，肯定按搜索结果的顺序看
  就算需要支持这个功能我可以通过限制页号最大值来避免深分页，页号太大后面的搜索结果不精准没有意义
- 自选页号功能：可以通过限制页号最大值来避免深分页，页号太大后面的搜索结果不精准没有意义
- 下一页功能：使用 Search After 实现

我们看看主流网站：

- 谷歌、百度：没有自定义跳页功能；支持自选页号，但根据测试百度网页版最多76页；支持下一页
- 淘宝：网页版支持自定义跳页但最大页是100，支持自选页号最大也是100页；手机端：通过下拉加载本质上就是下一页

因此`from+size`结合`Search After`是可以解决深分页问题的

# 🥇Master选举

## 节点类型

下文有

## 何时触发选举

情况1：活跃 master 节点数量小于法定票数
情况2：active master 挂掉

## 选举过程

![zoom=40](a217de37ad2473d987a10797fd890416.png)

法定票数：当选 Master 所需的最小票数，是可配置的，通常情况下为有效投票节点数过半

**脑裂问题**

部分节点不能与主节点正常通信，这些节点会选出一个新的主节点，集群就有了两个主节点，即脑裂

解决方案：`discovery.zen.minimum_master_nodes=N/2+1`，N为有效投票节点数；保证了只能有一部分节点的投票达到这个数字选出一个主节点，其他部分的投票均达不到这个数字选不出主节点

PS：集群通常应该有奇数个候选节点；如果是偶数的话，平分成两个部分任何一个部分都选不出主节点

# 🥇数据写入

数据写入均发生在主分片，写入完成后再同步到副本

**写数据过程**

1. 客户端选择一个节点发送请求
2. 这个节点就作为协调节点，根据文档 ID 进行路由，将请求转发给具备主分片的节点
3. 主分片节点写入成功之后，将数据同步到副本
4. 协调节点等到主分片和副本都写入成功之后，就返回响应结果给客户端

**写一致性**

一致性由参数`wait_for_active_shards`控制

- 默认值为1，即只需要主分片写入成功
- 可以设置为 all 或任何正整数，最大值为索引分片总数，如果设置为 2，就只需要一个副本分片写入成功即可；如果副本写入不成功，写操作必须等待并重试，超时时间是30秒

**写入原理**

![](d25b17eda3c3119158e35389637cf74a.png)

# 🥇读写性能调优

## 写入调优

以提升写入吞吐量和并发能力为目标，而非提升写入实时性

**增加 buffer 大小**本质上是减少 refresh 操作

**增加 refresh 时间间隔**
目的是减少 segment 的创建，减少 segment 的 merge 次数
这些操作都发生在 JVM，大量创建对象且对象长时间存活无法回收，有可能导致频繁的 full gc 甚至内存溢出
这个方案会降低写实时性

**增加 flush 时间间隔**目的是减小数据写入磁盘的频率，减小磁盘IO频率

**禁用交换分区swap**

**使用多个工作线程**为了使用集群的所有资源，应该使用多个线程发送数据

**避免使用稀疏数据**如果同一个 index 下存储含有不同字段的文档，会影响 ES 压缩文档的能力，导致查询效率降低

**异步写事务日志**如果允许数据丢失，可以设置异步写事务日志

## 查询调优

**避免单次召回大量数据**
搜索引擎最擅长的事情是从海量数据中查询少量相关文档，而非单次检索大量文档
非常不建议查询上万数据，如果有这样的需求，建议使用滚动查询

**避免单个文档过大**

**避免深度分页**

**使用 filter 代替 query**filter 不用计算评分

**避免使用脚本**相对于 DSL  而言，脚本性能差

**使用 keyword 类型**精准匹配，如果不分词，就用

# 🥇数据库的索引

数据库的索引一般都是以索引文件的形式存储在磁盘上，每次查询时加载到内存

Mysql B+树索引并不能直接找到行，只是找到行所在的页，通过把整页读入内存，再在内存中查找，因此索引树的高度决定了磁盘IO的次数

# 🥇节点类型

候选节点：配置了 master 的节点都能参与选举和投票，集群至少三个候选节点

仅投票节点：配置了 master 和 voting_only 的节点是仅投票节点，不参与选举，同时可作为是数据节点

主节点：

- 负责创建删除索引、分片的分配等
- 应避免主节点承载其他任务
- 非仅投票节点的候选节点都有可能被选为主节点

数据节点：保存索引文档分片，处理数据相关的操作

# 🥇评分算法

影响评分的三个维度：
![](d4e1cfb6f53aaa48a4ba37e8b068a14f.png)

早期评分算法是 TF/IDF

后期使用的 BM25 算法主要优化了词频对评分的影响，随着词频越来越高，对评分的影响越来越小越趋近于平缓

![](282a973c00204c3e0728be7a02ef5de4.png)

# 🥇高可用ES集群

ES的高可用性：集群可容忍部分节点宕机而保持服务的可用性和数据的完整性
1. 假如宕机的是Master，选举新的Master
2. Master尝试恢复故障节点
3. 主分片所在节点故障，Active Master 会将某个副本提升为主分片
4. Master将宕机期间丢失的数据同步到重启节点对应的分片上去，从而使服务恢复正常
5. 通过针对节点、分片的策略降低单故障点对整体服务产生的影响，例如分片分配感知中的举例

**分布式的优点**
- 高可用：集群可容忍部分节点宕机而保持服务的可用性和数据的完整性
- 高性能：负载均衡分担请求压力，大大提高集群的吞吐能力和并发能力
- 易扩展：当集群性能不足时，可以方便快速的扩展集群，不用停止服务

## 分片分配感知

通过自定义属性作为感知属性，如果 ES 知道哪些节点位于同一机架、同一机房或同一数据中心中，则它可以分离主副本分片，最大程度地降低故障时丢失数据的风险，需要做如下配置：

```shell
#配置节点属性
node.attr.rack_id:rack1
```

```shell
#集群级设置
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.awareness.attributes":"rack_id"
  }
}
```

举例：机房1有A、B两个节点，机房2有C节点，如果有三个分片0 1 2，主分片将平均分配到三个节点一个节点一个主分片，如果一个机房断电，必定有主分片不可用，虽然随后副本分片升级为主分片，但这个过程有短暂的时间将影响到数据写入，对生产环境有很大影响

解决方案：对机房1的节点做上面的配置，主分片将不被平均分配到三个节点，而是只分配到机房1的节点中，机房2中没有主分片存在，一个机房断电之后集群的可用率为50%

参考下图理解：
![](05aa612dca52163414657ca55796b778.png)

## 小规模集群

- 单节点集群不考虑
- 两节点集群，不能选主，不推荐
- 三节点集群，HA的最低配置，候选节点同时作为数据节点，业务量不大的话可以使用
- 多节点集群：三个候选节点（太多的话将影响选主的速度）；设置专用节点，候选节点与数据节点分开

## 大规模集群

单集群：

- 避免跨数据中心，ES 对网络和宽带需求较高
- 部署分片分配感知，降低单个区域（比如一个机架）节点宕机对整个服务造成影响

双区集群：

- 如果集群部署在两个区域比如两个机房，应该在每个机房部署不同的候选节点，服务可用率为50%
- 如果每个机房候选节点相同，一个机房断电将导致无法选主而使服务不可用（因此候选节点需要是奇数）

多区集群：选三个区域每个区域部署一个候选节点即可

# 🥇你们的集群架构、索引大小、分片多少、调优手段

集群架构：3个候选节点，3个数据节点
分片数量：3个分片
索引大小：没有计算过

调优手段：

1. 设计阶段调优
   合理的设置分词器
   映射的编写充分结合各个字段的属性，是否需要检索、是否需要存储等
2. **上文**读写性能调优
3. 根据需要部署**上文**高可用ES集群
4. 业务调优

# 🥇ES索引文档的过程

即文档写入ES，创建索引的过程，参考**上文**数据写入

# 🥇ES的搜索过程

可以参考**上文**深分页问题
1、各个分片先查询
2、协调节点聚合所有分片的查询结果返回给客户端
